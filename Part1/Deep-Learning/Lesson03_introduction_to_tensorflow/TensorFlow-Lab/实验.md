# TensorFlow 神经网络实验室

在本实验中，你将运用在“TensorFlow 入门”中学到的所有工具，为英文字母图像进行标记！你将使用的数据集是 [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html)，它包含了不同字体的 A 到 J 字母的图像。

以上图像是一些你将用于训练的数据示例。在训练网络后，你将把你的预测模型与测试数据进行对比。你的目标是在本实验结束时，对测试集进行预测，准确率至少达到 80%。让我们开始吧！

要开始本实验，你首先需要导入所有必要的模块。运行以下代码。如果运行成功，它将打印“所有模块已导入”。

```python
import hashlib
import os
import pickle
from urllib.request import urlretrieve

import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.utils import resample
from tqdm import tqdm
from zipfile import ZipFile

print('所有模块已导入。')
```

notMNIST 数据集对于许多计算机来说太大了，仅用于训练的图像就多达 500,000 张。你将使用这个数据的一个子集，每个标签（A-J）有 15,000 张图像。

```python
def download(url, file):
    """    从 url 下载文件    
    :param url: 文件的 URL    
    :param file: 本地文件路径    
    """
    if not os.path.isfile(file):
        print('正在下载 ' + file + '...')
        urlretrieve(url, file)
        print('下载完成')

# 下载训练和测试数据集
download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')
download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')

# 确保文件未损坏
assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\
        'notMNIST_train.zip 文件已损坏。删除该文件后重试。'
assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\
        'notMNIST_test.zip 文件已损坏。删除该文件后重试。'

# 等待所有文件下载完成
print('所有文件已下载。')
```

```python
def uncompress_features_labels(file):
    """    
    从 zip 文件中解压特征和标签    
    :param file
    : 要从中提取数据的 zip 文件
    """
    features = []
    labels = []

    with ZipFile(file) as zipf:
        # 进度条
        filenames_pbar = tqdm(zipf.namelist(), unit='files')

        # 从所有文件中获取特征和标签
        for filename in filenames_pbar:
            # 检查文件是否为目录
            if not filename.endswith('/'):
                with zipf.open(filename) as image_file:
                    image = Image.open(image_file)
                    image.load()
                    # 将图像数据加载为一维数组
                    # 我们使用 float32 以节省内存空间
                    feature = np.array(image, dtype=np.float32).flatten()

                # 从文件名中获取字母。这就是图像的字母。
                label = os.path.split(filename)[1][0]

                features.append(feature)
                labels.append(label)
    return np.array(features), np.array(labels)

# 从 zip 文件中获取特征和标签
train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')
test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')

# 限制数据量，以便在 Docker 容器中使用
docker_size_limit = 150000
train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)

# 设置特征工程的标志。这将防止你跳过重要步骤。
is_features_normal = False
is_labels_encod = False

# 等待所有特征和标签解压完成
print('所有特征和标签已解压。')
```

[image/mean_variance.png](https://kimi.moonshot.cn/chat/image/mean_variance.png)

## 问题 1

第一个问题涉及对训练和测试数据的特征进行归一化。

在 `normalize()` 函数中实现范围为 `a=0.1` 和 `b=0.9` 的最小 - 最大归一化。归一化后，输入数据中的像素值应范围从 0.1 到 0.9。

由于原始的 notMNIST 图像数据是 [灰度](https://en.wikipedia.org/wiki/Grayscale) 的，当前值范围从最小值 0 到最大值 255。

最小 - 最大归一化公式：

$$
X'=a+{\frac {\left(X-X_{\min }\right)\left(b-a\right)}{X_{\max }-X_{\min }}}
$$

*如果你在解决第一个问题时遇到困难，可以查看[解决方案](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/solutions.ipynb)。*

# 问题 1 - 为灰度图像数据实现最小 - 最大归一化

```python
def normalize_grayscale(image_data):
    """
    使用最小 - 最大归一化将图像数据归一化到 [0.1, 0.9] 范围
    :param image_data: 要归一化的图像数据
    :return: 归一化的图像数据
    """
    # TODO: 为灰度图像数据实现最小 - 最大归一化
    return (0.1 + (((image_data - 0) * (0.9 - 0.1)) / (255)))
```

### 不要修改以下代码，测试用例

```python
np.testing.assert_array_almost_equal(
    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),
    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,
     0.125098039216, 0.128235294118, 0.13137254902, 0.9],
    decimal=3)
np.testing.assert_array_almost_equal(
    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254, 255])),
    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,
     0.896862745098, 0.9])

if not is_features_normal:
    train_features = normalize_grayscale(train_features)
    test_features = normalize_grayscale(test_features)
    is_features_normal = True

print('测试通过！')


if not is_labels_encod:
    # 将标签转换为数字并应用独热编码
    encoder = LabelBinarizer()
    encoder.fit(train_labels)
    train_labels = encoder.transform(train_labels)
    test_labels = encoder.transform(test_labels)

    # 转换为 float32，以便在 TensorFlow 中与 float32 类型的特征相乘
    train_labels = train_labels.astype(np.float32)
    test_labels = test_labels.astype(np.float32)
    is_labels_encod = True

print('标签已独热编码')


assert is_features_normal, '你跳过了归一化特征的步骤'
assert is_labels_encod, '你跳过了独热编码标签的步骤'
# 获取随机化的训练和验证数据集
train_features, valid_features, train_labels, valid_labels = train_test_split(
    train_features,
    train_labels,
    test_size=0.05,
    random_state=832289)

print('训练特征和标签已随机化并分割。')
```

# 保存数据以便快速访问

```python
pickle_file = 'notMNIST.pickle'
if not os.path.isfile(pickle_file):
    print('正在保存数据到 pickle 文件...')
    try:
        with open('notMNIST.pickle', 'wb') as pfile:
            pickle.dump(
                {
                    'train_dataset': train_features,
                    'train_labels': train_labels,
                    'valid_dataset': valid_features,
                    'valid_labels': valid_labels,
                    'test_dataset': test_features,
                    'test_labels': test_labels,
                },
                pfile, pickle.HIGHEST_PROTOCOL)
    except Exception as e:
        print('无法将数据保存到', pickle_file, ':', e)
        raise

print('数据已缓存在 pickle 文件中。')
```

# 检查点

现在，你的所有进度都已保存到 pickle 文件中。如果你需要离开并稍后返回本实验，你无需从头开始。只需运行以下代码块，它将加载所有必要的数据和模块以继续进行。

Python复制

```python
%matplotlib inline

# 加载模块
import pickle
import math

import numpy as np
import tensorflow as tf
from tqdm import tqdm
import matplotlib.pyplot as plt

# 重新加载数据
pickle_file = 'notMNIST.pickle'
with open(pickle_file, 'rb') as f:
  pickle_data = pickle.load(f)
  train_features = pickle_data['train_dataset']
  train_labels = pickle_data['train_labels']
  valid_features = pickle_data['valid_dataset']
  valid_labels = pickle_data['valid_labels']
  test_features = pickle_data['test_dataset']
  test_labels = pickle_data['test_labels']
  del pickle_data  # 释放内存


print('数据和模块已加载。')
```

## 问题 2

为了让神经网络在你的数据上进行训练，你需要以下 [float32](https://www.tensorflow.org/resources/dims_types.html#data-types) 张量：

- `features`
  
  - 特征数据（`train_features`/`valid_features`/`test_features`）的占位符张量

- `labels`
  
  - 标签数据（`train_labels`/`valid_labels`/`test_labels`）的占位符张量

- `weights`
  
  - 随机数来自截断正态分布的变量张量。
    
    - 可以参考 [tf.truncated_normal() 文档](https://www.tensorflow.org/api_docs/python/constant_op.html#truncated_normal)。

- `biases`
  
  - 全零的变量张量。
    
    - 可以参考 [tf.zeros() 文档](https://www.tensorflow.org/api_docs/python/constant_op.html#zeros)。

*如果你在解决第二个问题时遇到困难，可以回顾课程中的“TensorFlow 线性函数”部分。如果仍然没有帮助，可以查看[解决方案](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/solutions.ipynb)。*

```python
features_count = 784
labels_count = 10

# TODO: 设置特征和标签张量
features = tf.placeholder(tf.float32)
labels = tf.placeholder(tf.float32)

# TODO: 设置权重和偏置张量
weights = tf.Variable(tf.truncated_normal((features_count, labels_count)))
biases = tf.Variable(tf.zeros(labels_count))
```

### 不要修改以下代码，测试用例

```python
from tensorflow.python.ops.variables import Variable

assert features._op.name.startswith('Placeholder'), 'features 必须是占位符'
assert labels._op.name.startswith('Placeholder'), 'labels 必须是占位符'
assert isinstance(weights, Variable), 'weights 必须是 TensorFlow 变量'
assert isinstance(biases, Variable), 'biases 必须是 TensorFlow 变量'

assert features._shape == None or (\
    features._shape.dims[0].value is None and\
    features._shape.dims[1].value in [None, 784]), '特征的形状不正确'
assert labels._shape  == None or (\
    labels._shape.dims[0].value is None and\
    labels._shape.dims[1].value in [None, 10]), '标签的形状不正确'
assert weights._variable._shape == (784, 10), '权重的形状不正确'
assert biases._variable._shape == (10), '偏置的形状不正确'

assert features._dtype == tf.float32, 'features 必须是 float32 类型'
assert labels._dtype == tf.float32, 'labels 必须是 float32 类型'

# 训练、验证和测试会话的输入字典
train_feed_dict = {features: train_features, labels: train_labels}
valid_feed_dict = {features: valid_features, labels: valid_labels}
test_feed_dict = {features: test_features, labels: test_labels}

# 线性函数 WX + b
logits = tf.matmul(features, weights) + biases

prediction = tf.nn.softmax(logits)

# 交叉熵
cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)

# 训练损失
loss = tf.reduce_mean(cross_entropy)

# 创建一个初始化所有变量的操作
init = tf.initialize_all_variables()

# 测试用例
with tf.Session() as session:
    session.run(init)
    session.run(loss, feed_dict=train_feed_dict)
    session.run(loss, feed_dict=valid_feed_dict)
    session.run(loss, feed_dict=test_feed_dict)
    biases_data = session.run(biases)

assert not np.count_nonzero(biases_data), 'biases 必须是零'
print('测试通过！')


# 确定预测是否正确
is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))
# 计算预测的准确率
accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))

print('准确率函数已创建。')
```

## 问题 3

以下是训练神经网络的 3 种参数配置。在每种配置中，其中一个参数有多个选项。对于每种配置，选择能够获得最佳准确率的选项。

参数配置：

配置 1

- **轮数** ：1

- **批量大小** ：
  
  - 2000
  
  - 1000
  
  - 500
  
  - 300
  
  - 50

- **学习率** ：0.01

配置 2

- **轮数** ：1

- **批量大小** ：100

- **学习率** ：
  
  - 0.8
  
  - 0.5
  
  - 0.1
  
  - 0.05
  
  - 0.01

配置 3

- **轮数** ：
  
  - 1
  
  - 2
  
  - 3
  
  - 4
  
  - 5

- **批量大小** ：100

- **学习率** ：0.2

代码将打印出损失和准确率图表，你可以看到神经网络的性能如何。

*如果你在解决第三个问题时遇到困难，可以查看[解决方案](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/solutions.ipynb)。*

```python
# TODO: 为每种配置找到最佳参数
epochs = 4
batch_size = 50
learning_rate = 0.2
```

### 不要修改以下代码，梯度下降

```python
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    

# 与验证集对比的准确率
validation_accuracy = 0.0

# 用于绘制损失和准确率图表的测量值
log_batch_step = 50
batches = []
loss_batch = []
train_acc_batch = []
valid_acc_batch = []

with tf.Session() as session:
    session.run(init)
    batch_count = int(math.ceil(len(train_features)/batch_size))

    for epoch_i in range(epochs):

        # 进度条
        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')

        # 训练循环
        for batch_i in batches_pbar:
            # 获取一批训练特征和标签
            batch_start = batch_i*batch_size
            batch_features = train_features[batch_start:batch_start + batch_size]
            batch_labels = train_labels[batch_start:batch_start + batch_size]

            # 运行优化器并获取损失
            _, l = session.run(
                [optimizer, loss],
                feed_dict={features: batch_features, labels: batch_labels})

          # 每50个批次记录一次日志
            if not batch_i % log_batch_step:
                # 计算训练和验证准确率
                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)
                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)

                # 记录批次
                previous_batch = batches[-1] if batches else 0
                batches.append(log_batch_step + previous_batch)
                loss_batch.append(l)
                train_acc_batch.append(training_accuracy)
                valid_acc_batch.append(validation_accuracy)

        # 使用验证数据检查准确率
        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)

# 绘制损失图
loss_plot = plt.subplot(211)
loss_plot.set_title('损失')
loss_plot.plot(batches, loss_batch, 'g')
loss_plot.set_xlim([batches[0], batches[-1]])
# 绘制准确率图
acc_plot = plt.subplot(212)
acc_plot.set_title('准确率')
acc_plot.plot(batches, train_acc_batch, 'r', label='训练准确率')
acc_plot.plot(batches, valid_acc_batch, 'x', label='验证准确率')
acc_plot.set_ylim([0, 1.0])
acc_plot.set_xlim([batches[0], batches[-1]])
acc_plot.legend(loc=4)
plt.tight_layout()
plt.show()

print('验证准确率为：{}'.format(validation_accuracy))
'.format(validation_accuracy))
```

## 测试

使用您在问题3中发现的最佳学习参数来设置epochs、batch_size和学习率。您将要把您的模型在预留数据集/测试数据上进行测试。这将给您一个很好的指标，显示模型在现实世界中的表现如何。您的测试准确率应至少达到80%。

```python
# TODO: 使用问题3中的最佳参数设置epochs、batch_size和learning_rate
epochs = 5
batch_size = 50
learning_rate = 0.3

### 不要修改以下内容 ###
# 对测试集的准确度进行测量
test_accuracy = 0.0

with tf.Session() as session:

    session.run(init)
    batch_count = int(math.ceil(len(train_features)/batch_size))

    for epoch_i in range(epochs):

        # 进度条
        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')

        # 训练周期
        for batch_i in batches_pbar:
            # 获取一批训练特征和标签
            batch_start = batch_i*batch_size
            batch_features = train_features[batch_start:batch_start + batch_size]
            batch_labels = train_labels[batch_start:batch_start + batch_size]

            # 运行优化器
            _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})

        # 使用测试数据检查准确度
        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)


assert test_accuracy >= 0.80, '测试准确度为{}，应等于或大于0.80'.format(test_accuracy)
print('干得好！测试准确度为{}'.format(test_accuracy))
```

干得好！你构建了一个单层的TensorFlow网络！然而，你还想构建更多层。毕竟，这是深度学习！在下一节中，你将开始满足你对更多层的需求。
